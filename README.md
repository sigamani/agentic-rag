# OSCAR: ConvFinQA: Finetuning and Evaluating a Chain-of-Thought LLM on Financial QA

This repository contains the full pipeline for fine-tuning a LLaMA-style model using chain-of-thought supervision on [ConvFinQA-style](https://github.com/sigamani/ConvFinQA2) financial reasoning tasks. It includes curriculum-generated data, a LoRA fine-tuning pipeline, and LangGraph-based program execution evaluation.

---

## ğŸ“ Project Structure
ConvFinQA3/
â”œâ”€â”€ configs/                # YAML-based training config
â”œâ”€â”€ data/                   # Curriculum or supervised examples
â”œâ”€â”€ eval/                   # Evaluation: retrieval + reasoning accuracy
â”œâ”€â”€ models/                 # Model + LoRA helpers
â”œâ”€â”€ retrieval/              # Hybrid retriever logic
â”œâ”€â”€ scripts/                # Training and inference scripts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ report.pdf

---

## ğŸ§  Objective

Fine-tune a small, instruction-tuned LLM on structured financial reasoning tasks (e.g., calculating ratios, extracting facts) with step-by-step supervision and evaluate its end-to-end performance using retrieval + reasoning metrics.

---

## ğŸ“¦ Installation

### 1. Create Environment

```bash
python3.10 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
(Or use conda if preferred)

â¸»

## ğŸ“ Dataset

We use curriculum_generated.jsonl â€“ a cleaned and CoT-augmented version of ConvFinQA-style examples, created via teacher LLMs and schema-based curation.

Each entry includes:

```
  {"question": "...",
  "context": "...",
  "program": "...",
  "answer": "...",
  "reasoning": "...",
  "table": "..."}   
```
â¸»

## ğŸ§ª Evaluation

Evaluation is two-fold:

1. Retrieval Accuracy (e.g. Recall@3)

```python eval/eval_retrieval_r@3.py```

2. Full LangGraph Execution Accuracy

```python eval/eval_langgraph.py```

This runs the modelâ€™s output program over retrieved tables and compares with gold answers.

â¸»

## ğŸ§¬ Fine-Tuning

âœ… With LoRA
```python scripts/fine_tune.py --config configs/config_finetune.yaml```

This will:
	â€¢	Load a quantised model (e.g. from Hugging Face or local GGUF)
	â€¢	Apply LoRA adapters
	â€¢	Train on the dataset with reasoning supervision
	â€¢	Save merged model + logs to checkpoints/

â¸»


## ğŸ” Inference

You can run a local inference pass using the same model:
```python scripts/run_inference.py --input example.json --checkpoint ./checkpoints/oscar-lora```
## âš™ï¸ Config (YAML)

All hyperparameters are stored in configs/config_finetune.yaml:

```
model_name_or_path: "meta-llama/Llama-2-7b-hf"
output_dir: "./checkpoints/oscar-lora"
dataset_path: "./data/curriculum_generated.jsonl"

num_train_epochs: 3
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 2e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
```
â¸»

ğŸ“š Citation & Credits
	â€¢	Built using Hugging Face Transformers, PEFT, and LangGraph.
	â€¢	Dataset adapted from ConvFinQA by TheFinAI + curriculum-generated CoT data.

â¸»

ğŸ› ï¸ Maintainer

Michael Sigamani
github.com/sigamani
Licensed under Apache 2.0

