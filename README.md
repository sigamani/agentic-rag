# ConvFinQA: Finetuning and Evaluating a Chain-of-Thought LLM on Financial QA
---

## ğŸ§  Objective

Fine-tune a small, instruction-tuned LLM on structured financial reasoning tasks (e.g., calculating ratios, extracting facts) with step-by-step supervision and evaluate its end-to-end performance using retrieval + reasoning metrics.

---

## ğŸ“¦ Installation

### 1. Create Environment

```bash
python3.10 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
(Or use conda if preferred)

â¸»

## ğŸ“ Dataset

We use curriculum_generated.jsonl â€“ a cleaned and CoT-augmented version of ConvFinQA-style examples, created via teacher LLMs and schema-based curation.

Each entry includes:

```
  {"question": "...",
  "context": "...",
  "program": "...",
  "answer": "...",
  "reasoning": "...",
  "table": "..."}   
```
â¸»

## ğŸ§ª Evaluation

Evaluation is two-fold:

1. Retrieval Accuracy (e.g. Recall@3)

```python eval/eval_retrieval_r@3.py```

2. Full LangGraph Execution Accuracy

```python eval/eval_langgraph.py```

This runs the modelâ€™s output program over retrieved tables and compares with gold answers.

â¸»

## ğŸ§¬ Fine-Tuning

âœ… With LoRA
```python scripts/fine_tune.py --config configs/config_finetune.yaml```

This will:
- Load a quantised model (e.g. from Hugging Face or local GGUF)
- Apply LoRA adapters
- Train on the dataset with reasoning supervision
- Save merged model + logs to checkpoints/

â¸»

## ğŸ” Inference

You can run a local inference pass using the same model:
```python scripts/run_inference.py --input example.json --checkpoint ./checkpoints/oscar-lora```
## âš™ï¸ Config (YAML)

All hyperparameters are stored in configs/config_finetune.yaml:

```
model_name_or_path: "meta-llama/Llama-2-7b-hf"
output_dir: "./checkpoints/oscar-lora"
dataset_path: "./data/curriculum_generated.jsonl"

num_train_epochs: 3
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 2e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
```
â¸»

ğŸ“š Citation & Credits
- Built using Hugging Face Transformers, PEFT, and LangGraph.
- Dataset adapted from ConvFinQA by TheFinAI + curriculum-generated CoT data.

â¸»

ğŸ› ï¸ Maintainer

Michael Sigamani
github.com/sigamani
Licensed under Apache 2.0

