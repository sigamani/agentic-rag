# OSCAR: ConvFinQA: Finetuning and Evaluating a Chain-of-Thought LLM on Financial QA

This repository contains the full pipeline for fine-tuning a LLaMA-style model using chain-of-thought supervision on [ConvFinQA-style](https://github.com/sigamani/ConvFinQA2) financial reasoning tasks. It includes curriculum-generated data, a LoRA fine-tuning pipeline, and LangGraph-based program execution evaluation.

---

## ğŸ“ Project Structure
ConvFinQA3/
â”œâ”€â”€ configs/                # YAML-based training config
â”œâ”€â”€ data/                   # Curriculum or supervised examples
â”œâ”€â”€ eval/                   # Evaluation: retrieval + reasoning accuracy
â”œâ”€â”€ models/                 # Model + LoRA helpers
â”œâ”€â”€ retrieval/              # Hybrid retriever logic
â”œâ”€â”€ scripts/                # Training and inference scripts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ report.pdf

---

## ğŸ§  Objective

Fine-tune a small, instruction-tuned LLM on structured financial reasoning tasks (e.g., calculating ratios, extracting facts) with step-by-step supervision and evaluate its end-to-end performance using retrieval + reasoning metrics.

---

## ğŸ“¦ Installation

### 1. Create Environment

```bash
python3.10 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
(Or use conda if preferred)

## ğŸ“ Dataset

We use curriculum_generated.jsonl â€“ a cleaned and CoT-augmented version of ConvFinQA-style examples, created via teacher LLMs and schema-based curation.

Each entry includes:

```
  {"question": "...",
  "context": "...",
  "program": "...",
  "answer": "...",
  "reasoning": "...",
  "table": "..."}   
```

## ğŸ§ª Evaluation

Evaluation is two-fold:

1. Retrieval Accuracy (e.g. Recall@3)

```python eval/eval_retrieval_r@3.py```

2. Full LangGraph Execution Accuracy

```python eval/eval_langgraph.py```
